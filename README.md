# ğŸš€  EOS-Bench
**A unified, high-performance, and extensible benchmark platform for **Earth Observation (EO) satellite scheduling**.**

Designed to tackle the complexities of large-scale constellation management, this project bridges the gap between traditional operations research and modern AI scheduling techniques. It features a complete pipeline from scenario generation (powered by Orekit) and diverse algorithm benchmarking, to interactive 3D visualization.

---

This project provides:

* ğŸ“¦ A scalable scenario generation framework (1â€“1000 satellites, 1hâ€“168h horizon)
* âš™ï¸ A unified constraint & evaluation model
* ğŸ§  Multiple scheduling paradigms:

  * MIP (Exact Optimization)
  * Heuristics
  * Meta-heuristics (SA / GA / ACO)
  * PPO Reinforcement Learning
* ğŸ“Š Standardized performance metrics
* ğŸŒ 3D dynamic visualization via Cesium

---

## ğŸ”¥ Key Features

* **Scalable Scenario Generation**: Generate realistic tasks and orbital mechanics for constellations ranging from 1 to 1000+ satellites over 1hâ€“168h horizons. Supports randomized mission distributions or precise JSON-based city targets.
* **High-Fidelity Constraint Modeling**: Natively supports **Agile and Non-Agile** satellites with piecewise attitude transition time models (Î”roll, Î”pitch, Î”yaw), power consumption, and data storage limits.
* **High-Performance Candidate Pool**: Employs truncated, diversified candidate sampling (Earliest, Center, Latest, Random) coupled with Early Accept and dynamic sampling techniques to keep complexity at O(T Â· K) for massive scales.
* **Unified Multi-Objective Optimization**: Standardized 0~1 normalized evaluation for Profit, Completion Rate, Timeliness, and Balance Degree.
* **Automated 3D Visualization**: One-click generation of CZML files for dynamic orbit and scheduling visualization via Cesium.

---

## ğŸ“‚ Project Structure

```
Satellite_Benchmark/
â”‚
â”œâ”€â”€ algorithms/          # Algorithm implementations & Factory
â”‚   â”œâ”€â”€ mip.py           # MILP formulation (PuLP)
â”‚   â”œâ”€â”€ heuristics.py    # TP, TCR, TM, BD heuristic schedulers
â”‚   â”œâ”€â”€ meta_*.py        # SA, GA, ACO implementations
â”‚   â”œâ”€â”€ ppo/             # PPO Policy, Agent, and RL training loops
â”‚   â”œâ”€â”€ candidate_pool.py# Diverse candidate window generation
â”‚   â””â”€â”€ objectives.py    # Multi-objective scoring models
â”‚
â”œâ”€â”€ core/                # Static domain models & Scenario generation
â”œâ”€â”€ schedulers/          # Constraint models, transition utils, RL env
â”œâ”€â”€ draw/                # Cesium 3D visualization tools
â”œâ”€â”€ input/               # Constellation setups & target data (e.g., cities.json)
â”œâ”€â”€ output/              # Generated scenarios, schedules, RL models, logs
â”‚
â”œâ”€â”€ main_generate.py     # Script to generate benchmark scenarios
â”œâ”€â”€ main_scheduler.py    # Main entry to run benchmark algorithms & train RL
â””â”€â”€ main_draw.py         # Launch local server for 3D visualization
```

---

## ğŸ“Š Evaluation Metrics

All algorithms are evaluated using unified metrics:

| Metric | Description          |
| ------ | -------------------- |
| TP     | Task Profit          |
| TCR    | Task Completion Rate |
| BD     | Balance Degree       |
| TM     | Timeliness Metric    |
| RT     | Runtime              |

---

## ğŸ›  Installation

### 1ï¸âƒ£ Install Java (Required for Orekit)

```bash
sudo apt update
sudo apt install -y openjdk-17-jdk
```

### 2ï¸âƒ£ Install Python Dependencies

Python 3.10 recommended.

```bash
pip install "orekit-jpype[jdk4py]" "jpype1==1.5.2"
pip install numpy pandas matplotlib scipy
pip install torch
```

---

## ğŸš€ Quick Start

### Step 1 â€” Generate Benchmark Scenarios

```bash
python main_generate.py
```

Scenarios are saved in:

```
output/Scenario_*.json
```

---

### Step 2 â€” Run Benchmark / Train RL

```bash
python main_scheduler.py
```

Outputs:

* Schedule results â†’ `output/schedules/`
* RL models â†’ `output/models/`
* Performance logs â†’ `runlog_*.txt`

---

### Step 3 â€” 3D Visualization

Edit scenario & schedule names in `main_draw.py`, then run:

```bash
python main_draw.py
```

A browser window will open with dynamic 3D visualization.

---

## ğŸ§  Reinforcement Learning (PPO)

The PPO module supports:

* Multi-objective weighted training
* Periodic checkpoint saving
* Automatic resume training
* Independent train/test mode
* Scenario resampling control (every N episodes)

Saved models include:

* Policy weights
* Optimizer state
* Episode index
* RNG states (for reproducibility)

---

## â• Adding New Algorithms

1. Implement your algorithm in `algorithms/`
2. Follow the unified interface
3. Register in `factory.py`
4. Add to `algo_specs` in `main_scheduler.py`

It will automatically integrate into batch benchmarking.

---

## ğŸ“ˆ Designed For

* Algorithm comparison studies
* Scalability analysis
* RL vs classical optimization research
* Reproducible scheduling experiments
* Engineering validation

---

## ğŸ“Œ Notes

* Visibility computation is powered by **Orekit**
* All algorithms share a unified constraint model
* Designed for large-scale EO constellation scheduling research

---


